{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe46b5ba",
   "metadata": {},
   "source": [
    "# Topic Modelling with Latend Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a11e0",
   "metadata": {},
   "source": [
    "### Step 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500ad7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4920\\3023278298.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of documents is: 300000\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset from a csv\n",
    "import pandas as pd\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n",
    "data_text = data[:300000][['headline_text']];\n",
    "data_text['index'] = data_text.index\n",
    "\n",
    "documents = data_text\n",
    "\n",
    "# Total number of documents\n",
    "print(f'The total number of documents is: {len(documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d4be0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922f3346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b814ab",
   "metadata": {},
   "source": [
    "### Step 2. Imports and data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c9a62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a735a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer example\n",
    "print(WordNetLemmatizer().lemmatize('went', pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39349653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemmer example\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "\n",
    "pd.DataFrame(data={'original word':original_words, 'stemmed':singles })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec0dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lematization on the entire dataset\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e52cb352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['aba', 'decides', 'against', 'community', 'broadcasting', 'licence']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['decid', 'communiti', 'broadcast', 'licenc']\n"
     ]
    }
   ],
   "source": [
    "# Document example after preprocessing\n",
    "\n",
    "document_num = 0\n",
    "doc_sample = documents[documents['index'] == 0].values[0][0]\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "432d6c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [decid, communiti, broadcast, licenc]\n",
       "1                        [wit, awar, defam]\n",
       "2    [call, infrastructur, protect, summit]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess headlines\n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c34cc",
   "metadata": {},
   "source": [
    "### Step 3. Bag of Words approach on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07d190d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of word counts\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e0459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove too rare and too common words from the dictionary\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "140e7b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the bag of words corpus using the word counts\n",
    "bow_corpus = [dictionary.doc2bow(document) for document in processed_docs]\n",
    "bow_corpus[document_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8ad984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"broadcast\") appears 1 time.\n",
      "Word 1 (\"communiti\") appears 1 time.\n",
      "Word 2 (\"decid\") appears 1 time.\n",
      "Word 3 (\"licenc\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Bag of words document example\n",
    "bow_doc_4310 = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ddd9fb",
   "metadata": {},
   "source": [
    "### Step 4. TF-IDF approach on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05b73598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "# tfidf model and corpus\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2463f167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5959813347777092),\n",
      " (1, 0.39204529549491984),\n",
      " (2, 0.48531419274988147),\n",
      " (3, 0.5055461098578569)]\n"
     ]
    }
   ],
   "source": [
    "# tfidf scores\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67ce76",
   "metadata": {},
   "source": [
    "### Step 5. LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ba91dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LDA model using Multicore approach from the gensim library.\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 10, id2word = dictionary, passes = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e19a1780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0.024*\"call\" + 0.021*\"probe\" + 0.019*\"talk\" + 0.017*\"say\" + 0.017*\"hold\" + 0.016*\"work\" + 0.014*\"chief\" + 0.014*\"leader\" + 0.012*\"inquiri\" + 0.011*\"launch\" \n",
      "Words: 0\n",
      "\n",
      "\n",
      "Topic: 0.027*\"claim\" + 0.020*\"govt\" + 0.018*\"minist\" + 0.016*\"nation\" + 0.016*\"worker\" + 0.015*\"labor\" + 0.014*\"protest\" + 0.014*\"union\" + 0.014*\"defend\" + 0.014*\"school\" \n",
      "Words: 1\n",
      "\n",
      "\n",
      "Topic: 0.038*\"report\" + 0.029*\"hospit\" + 0.019*\"coast\" + 0.016*\"open\" + 0.015*\"gold\" + 0.012*\"deal\" + 0.012*\"guilti\" + 0.011*\"sign\" + 0.011*\"find\" + 0.010*\"bushfir\" \n",
      "Words: 2\n",
      "\n",
      "\n",
      "Topic: 0.028*\"fund\" + 0.026*\"govt\" + 0.022*\"water\" + 0.020*\"boost\" + 0.016*\"servic\" + 0.014*\"urg\" + 0.014*\"health\" + 0.013*\"farmer\" + 0.012*\"price\" + 0.011*\"drought\" \n",
      "Words: 3\n",
      "\n",
      "\n",
      "Topic: 0.016*\"australia\" + 0.016*\"lead\" + 0.015*\"world\" + 0.014*\"win\" + 0.012*\"final\" + 0.011*\"test\" + 0.010*\"aussi\" + 0.010*\"england\" + 0.009*\"play\" + 0.008*\"clash\" \n",
      "Words: 4\n",
      "\n",
      "\n",
      "Topic: 0.064*\"polic\" + 0.035*\"charg\" + 0.031*\"court\" + 0.027*\"face\" + 0.019*\"jail\" + 0.018*\"drug\" + 0.017*\"murder\" + 0.016*\"accus\" + 0.014*\"case\" + 0.013*\"arrest\" \n",
      "Words: 5\n",
      "\n",
      "\n",
      "Topic: 0.016*\"year\" + 0.016*\"market\" + 0.016*\"help\" + 0.015*\"record\" + 0.014*\"law\" + 0.013*\"trade\" + 0.013*\"sale\" + 0.013*\"high\" + 0.013*\"assault\" + 0.011*\"abus\" \n",
      "Words: 6\n",
      "\n",
      "\n",
      "Topic: 0.037*\"kill\" + 0.028*\"crash\" + 0.018*\"death\" + 0.017*\"road\" + 0.016*\"polic\" + 0.015*\"attack\" + 0.014*\"die\" + 0.013*\"investig\" + 0.013*\"fear\" + 0.013*\"dead\" \n",
      "Words: 7\n",
      "\n",
      "\n",
      "Topic: 0.062*\"plan\" + 0.049*\"council\" + 0.019*\"consid\" + 0.018*\"concern\" + 0.015*\"govt\" + 0.014*\"group\" + 0.012*\"resid\" + 0.012*\"mayor\" + 0.011*\"develop\" + 0.010*\"park\" \n",
      "Words: 8\n",
      "\n",
      "\n",
      "Topic: 0.029*\"miss\" + 0.029*\"continu\" + 0.022*\"forc\" + 0.018*\"search\" + 0.017*\"iraq\" + 0.013*\"clear\" + 0.012*\"troop\" + 0.011*\"fish\" + 0.011*\"rescu\" + 0.010*\"build\" \n",
      "Words: 9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each topic explore the words occurence for that topic and their relative weights\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(topic, idx ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c327b9",
   "metadata": {},
   "source": [
    "### Step 6. LDA using TF-IDF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7031ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LDA model using Multicore approach from the gensim library.\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics = 10, id2word = dictionary, passes = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d1f5281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.008*\"award\" + 0.008*\"nuclear\" + 0.008*\"korea\" + 0.007*\"talk\" + 0.007*\"deal\" + 0.007*\"downer\" + 0.006*\"iran\" + 0.006*\"murray\" + 0.006*\"china\" + 0.006*\"trade\"\n",
      "\n",
      "\n",
      "Topic: 1 Word: 0.013*\"price\" + 0.012*\"rise\" + 0.011*\"market\" + 0.009*\"rain\" + 0.007*\"rate\" + 0.007*\"farmer\" + 0.007*\"high\" + 0.006*\"water\" + 0.006*\"profit\" + 0.006*\"drought\"\n",
      "\n",
      "\n",
      "Topic: 2 Word: 0.012*\"toll\" + 0.010*\"bird\" + 0.008*\"fish\" + 0.007*\"illeg\" + 0.006*\"rebel\" + 0.006*\"death\" + 0.005*\"road\" + 0.005*\"crackdown\" + 0.005*\"news\" + 0.005*\"human\"\n",
      "\n",
      "\n",
      "Topic: 3 Word: 0.025*\"crash\" + 0.015*\"polic\" + 0.012*\"investig\" + 0.011*\"die\" + 0.010*\"accid\" + 0.010*\"victim\" + 0.009*\"fatal\" + 0.008*\"blaze\" + 0.007*\"death\" + 0.007*\"plane\"\n",
      "\n",
      "\n",
      "Topic: 4 Word: 0.024*\"charg\" + 0.021*\"court\" + 0.020*\"polic\" + 0.016*\"murder\" + 0.014*\"jail\" + 0.014*\"drug\" + 0.012*\"face\" + 0.010*\"assault\" + 0.009*\"accus\" + 0.009*\"search\"\n",
      "\n",
      "\n",
      "Topic: 5 Word: 0.011*\"final\" + 0.009*\"gold\" + 0.008*\"open\" + 0.008*\"drink\" + 0.007*\"coast\" + 0.007*\"world\" + 0.005*\"game\" + 0.005*\"hewitt\" + 0.005*\"age\" + 0.004*\"south\"\n",
      "\n",
      "\n",
      "Topic: 6 Word: 0.017*\"council\" + 0.015*\"plan\" + 0.011*\"govt\" + 0.011*\"water\" + 0.007*\"urg\" + 0.007*\"mayor\" + 0.007*\"develop\" + 0.006*\"fund\" + 0.006*\"group\" + 0.006*\"communiti\"\n",
      "\n",
      "\n",
      "Topic: 7 Word: 0.019*\"closer\" + 0.009*\"england\" + 0.008*\"tiger\" + 0.007*\"aussi\" + 0.006*\"victori\" + 0.006*\"lead\" + 0.006*\"australia\" + 0.006*\"test\" + 0.006*\"black\" + 0.005*\"blue\"\n",
      "\n",
      "\n",
      "Topic: 8 Word: 0.020*\"iraq\" + 0.018*\"kill\" + 0.011*\"troop\" + 0.010*\"bomb\" + 0.010*\"iraqi\" + 0.008*\"attack\" + 0.008*\"soldier\" + 0.007*\"blast\" + 0.007*\"baghdad\" + 0.007*\"terror\"\n",
      "\n",
      "\n",
      "Topic: 9 Word: 0.012*\"govt\" + 0.011*\"health\" + 0.010*\"fund\" + 0.008*\"union\" + 0.007*\"labor\" + 0.007*\"urg\" + 0.006*\"servic\" + 0.006*\"plan\" + 0.006*\"indigen\" + 0.006*\"opposit\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each topic explore the words occurence for that topic and their relative weights\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475ac28",
   "metadata": {},
   "source": [
    "### Step 7. Performance evaluation on Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ad1120c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decid', 'communiti', 'broadcast', 'licenc']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example document\n",
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c0bde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8199866414070129\t \n",
      "Topic: 0.062*\"plan\" + 0.049*\"council\" + 0.019*\"consid\" + 0.018*\"concern\" + 0.015*\"govt\" + 0.014*\"group\" + 0.012*\"resid\" + 0.012*\"mayor\" + 0.011*\"develop\" + 0.010*\"park\"\n",
      "\n",
      "Score: 0.020001880824565887\t \n",
      "Topic: 0.028*\"fund\" + 0.026*\"govt\" + 0.022*\"water\" + 0.020*\"boost\" + 0.016*\"servic\" + 0.014*\"urg\" + 0.014*\"health\" + 0.013*\"farmer\" + 0.012*\"price\" + 0.011*\"drought\"\n",
      "\n",
      "Score: 0.020001878961920738\t \n",
      "Topic: 0.016*\"australia\" + 0.016*\"lead\" + 0.015*\"world\" + 0.014*\"win\" + 0.012*\"final\" + 0.011*\"test\" + 0.010*\"aussi\" + 0.010*\"england\" + 0.009*\"play\" + 0.008*\"clash\"\n",
      "\n",
      "Score: 0.02000136487185955\t \n",
      "Topic: 0.024*\"call\" + 0.021*\"probe\" + 0.019*\"talk\" + 0.017*\"say\" + 0.017*\"hold\" + 0.016*\"work\" + 0.014*\"chief\" + 0.014*\"leader\" + 0.012*\"inquiri\" + 0.011*\"launch\"\n",
      "\n",
      "Score: 0.02000136487185955\t \n",
      "Topic: 0.027*\"claim\" + 0.020*\"govt\" + 0.018*\"minist\" + 0.016*\"nation\" + 0.016*\"worker\" + 0.015*\"labor\" + 0.014*\"protest\" + 0.014*\"union\" + 0.014*\"defend\" + 0.014*\"school\"\n",
      "\n",
      "Score: 0.02000136487185955\t \n",
      "Topic: 0.038*\"report\" + 0.029*\"hospit\" + 0.019*\"coast\" + 0.016*\"open\" + 0.015*\"gold\" + 0.012*\"deal\" + 0.012*\"guilti\" + 0.011*\"sign\" + 0.011*\"find\" + 0.010*\"bushfir\"\n",
      "\n",
      "Score: 0.02000136487185955\t \n",
      "Topic: 0.064*\"polic\" + 0.035*\"charg\" + 0.031*\"court\" + 0.027*\"face\" + 0.019*\"jail\" + 0.018*\"drug\" + 0.017*\"murder\" + 0.016*\"accus\" + 0.014*\"case\" + 0.013*\"arrest\"\n",
      "\n",
      "Score: 0.02000136487185955\t \n",
      "Topic: 0.016*\"year\" + 0.016*\"market\" + 0.016*\"help\" + 0.015*\"record\" + 0.014*\"law\" + 0.013*\"trade\" + 0.013*\"sale\" + 0.013*\"high\" + 0.013*\"assault\" + 0.011*\"abus\"\n",
      "\n",
      "Score: 0.02000136487185955\t \n",
      "Topic: 0.037*\"kill\" + 0.028*\"crash\" + 0.018*\"death\" + 0.017*\"road\" + 0.016*\"polic\" + 0.015*\"attack\" + 0.014*\"die\" + 0.013*\"investig\" + 0.013*\"fear\" + 0.013*\"dead\"\n",
      "\n",
      "Score: 0.02000136487185955\t \n",
      "Topic: 0.029*\"miss\" + 0.029*\"continu\" + 0.022*\"forc\" + 0.018*\"search\" + 0.017*\"iraq\" + 0.013*\"clear\" + 0.012*\"troop\" + 0.011*\"fish\" + 0.011*\"rescu\" + 0.010*\"build\"\n"
     ]
    }
   ],
   "source": [
    "document_num = 0\n",
    "# Doment test with the LDA model\n",
    "for index, score in sorted(lda_model[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d1e38a",
   "metadata": {},
   "source": [
    "### Step 8. Performance evaluation on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "556a97a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4290580153465271\t \n",
      "Topic: 0.012*\"toll\" + 0.010*\"bird\" + 0.008*\"fish\" + 0.007*\"illeg\" + 0.006*\"rebel\" + 0.006*\"death\" + 0.005*\"road\" + 0.005*\"crackdown\" + 0.005*\"news\" + 0.005*\"human\"\n",
      "\n",
      "Score: 0.4108455777168274\t \n",
      "Topic: 0.017*\"council\" + 0.015*\"plan\" + 0.011*\"govt\" + 0.011*\"water\" + 0.007*\"urg\" + 0.007*\"mayor\" + 0.007*\"develop\" + 0.006*\"fund\" + 0.006*\"group\" + 0.006*\"communiti\"\n",
      "\n",
      "Score: 0.020026210695505142\t \n",
      "Topic: 0.020*\"iraq\" + 0.018*\"kill\" + 0.011*\"troop\" + 0.010*\"bomb\" + 0.010*\"iraqi\" + 0.008*\"attack\" + 0.008*\"soldier\" + 0.007*\"blast\" + 0.007*\"baghdad\" + 0.007*\"terror\"\n",
      "\n",
      "Score: 0.020011257380247116\t \n",
      "Topic: 0.012*\"govt\" + 0.011*\"health\" + 0.010*\"fund\" + 0.008*\"union\" + 0.007*\"labor\" + 0.007*\"urg\" + 0.006*\"servic\" + 0.006*\"plan\" + 0.006*\"indigen\" + 0.006*\"opposit\"\n",
      "\n",
      "Score: 0.020010722801089287\t \n",
      "Topic: 0.019*\"closer\" + 0.009*\"england\" + 0.008*\"tiger\" + 0.007*\"aussi\" + 0.006*\"victori\" + 0.006*\"lead\" + 0.006*\"australia\" + 0.006*\"test\" + 0.006*\"black\" + 0.005*\"blue\"\n",
      "\n",
      "Score: 0.02001035399734974\t \n",
      "Topic: 0.011*\"final\" + 0.009*\"gold\" + 0.008*\"open\" + 0.008*\"drink\" + 0.007*\"coast\" + 0.007*\"world\" + 0.005*\"game\" + 0.005*\"hewitt\" + 0.005*\"age\" + 0.004*\"south\"\n",
      "\n",
      "Score: 0.020010080188512802\t \n",
      "Topic: 0.024*\"charg\" + 0.021*\"court\" + 0.020*\"polic\" + 0.016*\"murder\" + 0.014*\"jail\" + 0.014*\"drug\" + 0.012*\"face\" + 0.010*\"assault\" + 0.009*\"accus\" + 0.009*\"search\"\n",
      "\n",
      "Score: 0.020009547472000122\t \n",
      "Topic: 0.008*\"award\" + 0.008*\"nuclear\" + 0.008*\"korea\" + 0.007*\"talk\" + 0.007*\"deal\" + 0.007*\"downer\" + 0.006*\"iran\" + 0.006*\"murray\" + 0.006*\"china\" + 0.006*\"trade\"\n",
      "\n",
      "Score: 0.02000931091606617\t \n",
      "Topic: 0.025*\"crash\" + 0.015*\"polic\" + 0.012*\"investig\" + 0.011*\"die\" + 0.010*\"accid\" + 0.010*\"victim\" + 0.009*\"fatal\" + 0.008*\"blaze\" + 0.007*\"death\" + 0.007*\"plane\"\n",
      "\n",
      "Score: 0.02000894397497177\t \n",
      "Topic: 0.013*\"price\" + 0.012*\"rise\" + 0.011*\"market\" + 0.009*\"rain\" + 0.007*\"rate\" + 0.007*\"farmer\" + 0.007*\"high\" + 0.006*\"water\" + 0.006*\"profit\" + 0.006*\"drought\"\n"
     ]
    }
   ],
   "source": [
    "# Doment test with the LDA model\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e304e1",
   "metadata": {},
   "source": [
    "### Step 9. Testing model on an unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "739a01c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.22005243599414825\t Topic: 0.016*\"year\" + 0.016*\"market\" + 0.016*\"help\" + 0.015*\"record\" + 0.014*\"law\"\n",
      "Score: 0.22003579139709473\t Topic: 0.037*\"kill\" + 0.028*\"crash\" + 0.018*\"death\" + 0.017*\"road\" + 0.016*\"polic\"\n",
      "Score: 0.22003096342086792\t Topic: 0.062*\"plan\" + 0.049*\"council\" + 0.019*\"consid\" + 0.018*\"concern\" + 0.015*\"govt\"\n",
      "Score: 0.21982374787330627\t Topic: 0.028*\"fund\" + 0.026*\"govt\" + 0.022*\"water\" + 0.020*\"boost\" + 0.016*\"servic\"\n",
      "Score: 0.020011626183986664\t Topic: 0.029*\"miss\" + 0.029*\"continu\" + 0.022*\"forc\" + 0.018*\"search\" + 0.017*\"iraq\"\n",
      "Score: 0.020011186599731445\t Topic: 0.024*\"call\" + 0.021*\"probe\" + 0.019*\"talk\" + 0.017*\"say\" + 0.017*\"hold\"\n",
      "Score: 0.020008552819490433\t Topic: 0.027*\"claim\" + 0.020*\"govt\" + 0.018*\"minist\" + 0.016*\"nation\" + 0.016*\"worker\"\n",
      "Score: 0.020008552819490433\t Topic: 0.038*\"report\" + 0.029*\"hospit\" + 0.019*\"coast\" + 0.016*\"open\" + 0.015*\"gold\"\n",
      "Score: 0.020008552819490433\t Topic: 0.016*\"australia\" + 0.016*\"lead\" + 0.015*\"world\" + 0.014*\"win\" + 0.012*\"final\"\n",
      "Score: 0.020008552819490433\t Topic: 0.064*\"polic\" + 0.035*\"charg\" + 0.031*\"court\" + 0.027*\"face\" + 0.019*\"jail\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"My favorite sports activities are running and swimming.\"\n",
    "\n",
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
